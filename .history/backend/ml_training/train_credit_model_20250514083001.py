import numpy as np # arrays
import pandas as pd # data manipulation
from sklearn.model_selection import train_test_split # splitting data
from sklearn.ensemble import RandomForestClassifier # classification model
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder # for scaling and encoding features
from sklearn.compose import ColumnTransformer # applying different transforms to different columns
from sklearn.pipeline import Pipeline # chaining steps together
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score # performance metrics
import joblib # used for saving the model and preprocessing objects

print("--- Credit Assessment Model Training Script (v2 - Corrected) ---") # script start message

# load data generated by the merging script
csv_filename = "merged_credit_data_v2.csv" # name of input file
try:
    print(f"Loading {csv_filename}...")
    df = pd.read_csv(csv_filename) # load csv into a pandas dataframe
    print("Dataset loaded successfully.")
    print("Shape of loaded data:", df.shape) # show rows and columns
    print("Sample of loaded data:")
    print(df.head()) # show first few rows
    print("\nColumn Names:", df.columns.tolist()) # show all column names
    print("\nData Info:")
    df.info() # display data types and non-null counts

except FileNotFoundError:
    # handle case where the input file doesn't exist
    print(f"ERROR: {csv_filename} not found.")
    print("Please ensure you ran the LATEST merging script (base.py) successfully first.")
    exit() # stop script
except Exception as e:
    # handle other potential errors during file loading
    print(f"ERROR loading data from {csv_filename}: {e}")
    exit() # stop script

# define target variable and features
target_column = 'loan_status' # name of the column to predict

if target_column not in df.columns:
    # check if target column exists in the loaded data
    print(f"ERROR: Target column '{target_column}' not found in {csv_filename}.")
    print("Available columns:", df.columns.tolist())
    exit() # stop if target column is missing

# verify target is binary 0/1
# check distinct values after potential nan drop below if needed
target_values = df[target_column].dropna().unique() # get unique values in the target column (ignoring nans for now)
if not all(v in [0, 1] for v in target_values):
    # warn if target column isn't just 0s and 1s
    print(f"Warning: Target column '{target_column}' doesn't seem to be strictly binary (0/1). Found values: {target_values}")

# separate features (x) and target (y)
y = df[target_column] # assign target column to y
X = df.drop(target_column, axis=1) # assign all other columns to x (features)
print(f"\nTarget variable '{target_column}' separated.")
print("Features (X) shape:", X.shape) # show dimensions of features
print("Target (y) shape:", y.shape) # show dimensions of target
print("Columns in features X:", X.columns.tolist()) # verify all expected features are here

# handle missing data
print(f"\nChecking for missing values before split...")
if X.isnull().any().any():
    # check if there are any missing values in the features (x)
    print("Warning: Found unexpected NaNs in features (X) after loading merged file. Dropping rows...")
    original_rows_X = X.shape[0] # store original number of rows
    rows_to_drop_X_index = X[X.isnull().any(axis=1)].index # get index of rows with nans in x
    X = X.dropna() # remove rows with nans from x
    y = y.drop(rows_to_drop_X_index) # remove corresponding rows from y to keep alignment
    print(f"Dropped {original_rows_X - X.shape[0]} rows from X (and y) due to NaNs.")
else: print("No missing values found in features (X).") # message if no nans found in x
if y.isnull().any():
    # check if there are any missing values in the target (y)
     print("Warning: Found unexpected NaNs in target variable (y). Dropping rows...")
     original_rows_y = y.shape[0] # store original number of rows in y
     y = y.dropna() # remove rows with nans from y
     X = X.loc[y.index] # keep only the corresponding rows in x
     print(f"Dropped {original_rows_y - y.shape[0]} rows from y (and X) due to NaNs.")
else: print("No missing values found in target (y).") # message if no nans found in y

# final shape check
print("Final shapes before split - X:", X.shape, "y:", y.shape) # display shapes after potential nan removal
if X.shape[0] == 0:
    # check if any data is left
    print("ERROR: No data remaining after handling missing values.")
    exit() # stop if no data remains

# identify feature types
# list of categorical features ready for encoding
categorical_features = ['employment_status', 'cb_person_default_on_file'] # from users/credit_data
# list of numerical features (need scaling)
numerical_features = [
    'credit_utilization_ratio',     # from scoring
    'payment_history',              # from scoring
    'loan_term',                    # from scoring
    'person_income',                # from risk
    'loan_amnt',                    # from risk
    'loan_percent_income',          # from risk
    'cb_person_cred_hist_length',   # from risk/users
    'original_loan_amount',
]
# --- ---------------------------------------------------------------------------------------------------------- ---

print("\nIdentified Feature Types:")
print("Categorical:", categorical_features) # show categorical list
print("Numerical:", numerical_features) # show numerical list

# ensure lists cover all columns in x
# compare defined features with actual columns in x dataframe
all_defined_features = set(numerical_features + categorical_features) # combine defined features into a set
actual_X_columns = set(X.columns) # get actual column names from x as a set

missing_in_X = all_defined_features - actual_X_columns # find features in lists but not in x
if missing_in_X:
    # error if defined features are missing from the actual data
    print(f"ERROR: Features defined in lists but MISSING from the loaded data X: {missing_in_X}")
    exit() # stop script

extra_in_X = actual_X_columns - all_defined_features # find features in x but not in the lists
if extra_in_X:
    # error if data has columns not accounted for in the feature lists
    print(f"ERROR: Features found in the loaded data X but NOT defined in lists: {extra_in_X}")
    exit() # stop script
# --- ------------------------------------------------- ---


# train-test split
print("\nSplitting data into Training (80%) and Testing (20%) sets...")
# split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, # features and target data
    test_size=0.2,       # 20% held out for testing
    random_state=42,     # ensures reproducibility of the split
    stratify=y           # keeps class proportions same in train/test
)
print("Splitting complete.")
print("X_train shape:", X_train.shape, "| y_train shape:", y_train.shape) # show shapes of training sets
print("X_test shape:", X_test.shape, "| y_test shape:", y_test.shape) # show shapes of testing sets
if X_train.shape[0] == 0 or X_test.shape[0] == 0:
    # check if splits resulted in empty sets
     print("ERROR: Train or Test set is empty after split. Check input data size.")
     exit() # stop if sets are empty


# preprocessing pipeline (scaling and encoding)
print("\nSetting up preprocessing pipeline...")
# transformer for numerical features: scale to range [0, 1]
numerical_transformer = Pipeline(steps=[
    ('scaler', MinMaxScaler()) # applies min-max scaling
])

# transformer for categorical features: convert categories into one-hot vectors
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # applies one-hot encoding, ignores categories not seen in training
])

# use columntransformer to apply different transformers to different columns
# this ensures the correct features are selected and processed
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),   # apply scaling to numerical list
        ('cat', categorical_transformer, categorical_features) # apply one-hot to categorical list
    ],
    remainder='passthrough' # if any columns in x aren't in num/cat lists pass them through unchanged
)
print("Preprocessor configured.")


# model definition & training pipeline
print("\nTraining Random Forest Classifier...")
# defining the random forest model
rf_model = RandomForestClassifier(
    n_estimators=100,      # number of trees in the forest
    random_state=42,       # ensures reproducibility of the model training
    class_weight='balanced', # adjusts weights for imbalanced classes (like loan default)
    n_jobs=-1              # use all available cpu cores for training
)

# create the full pipeline: preprocessing -> classifier
# applies preprocessing and then trains the model
full_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor), # first step: preprocess the data
    ('classifier', rf_model)        # second step: train the classifier
])

# train the entire pipeline on the training data
# preprocessing steps are fit only on x_train and then transform x_train
# classifier is trained on the preprocessed x_train
full_pipeline.fit(X_train, y_train)
print("Training complete.")


# model evaluation on test set
print("\nEvaluating model on the Test set...")
# use the pipeline to predict on the test data
# pipeline automatically applies the fitted preprocessing steps to x_test
y_pred = full_pipeline.predict(X_test) # get class predictions (0 or 1)
y_pred_proba = full_pipeline.predict_proba(X_test)[:, 1] # get probability of the positive class (1)
# -------------------------------------------------------------------

# calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred) # calculate accuracy
report = classification_report(y_test, y_pred) # get precision, recall, f1-score
try:
    # auc requires probabilities and at least two classes in y_test
    auc = roc_auc_score(y_test, y_pred_proba) # calculate area under the roc curve
    print(f"Test Set AUC: {auc:.4f}")
except ValueError as e:
    # handle error if only one class represented in y_test
    print(f"Could not calculate AUC (likely only one class in test set labels): {e}")
    auc = "N/A"

print(f"Test Set Accuracy: {accuracy:.4f}") # display accuracy
print("\nTest Set Classification Report:\n", report) # display classification report


# saving trained pipeline
print("\nSaving the trained model pipeline...")
model_filename = 'credit_model_v2.joblib' # filename for the saved pipeline
try:
    # save the entire pipeline object (preprocessor + classifier)
    joblib.dump(full_pipeline, model_filename) # save the pipeline to disk
    print(f"✅ Trained Pipeline saved successfully to {model_filename}")
except Exception as e:
    # handle errors during saving
     print(f"ERROR saving model pipeline: {e}")

print("\n--- Training Script Finished ---") # script ended message