# ========================================================
# 02_model_training_and_evaluation.py - COMPLETE/CORRECTED
# Uses output from base.py v2 (merged_credit_data_v2.csv)
# ========================================================

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import joblib # Used for saving the model and preprocessing objects

print("--- Credit Assessment Model Training Script (v2 - Corrected) ---")

# 1. Load Data generated by the merging script
# --- Ensures it loads the correctly generated file ---
csv_filename = "merged_credit_data_v2.csv"
try:
    print(f"Loading {csv_filename}...")
    df = pd.read_csv(csv_filename)
    print("Dataset loaded successfully.")
    print("Shape of loaded data:", df.shape)
    print("Sample of loaded data:")
    print(df.head())
    print("\nColumn Names:", df.columns.tolist())
    print("\nData Info:")
    df.info()

except FileNotFoundError:
    print(f"ERROR: {csv_filename} not found.")
    print("Please ensure you ran the LATEST merging script (base.py) successfully first.")
    exit()
except Exception as e:
    print(f"ERROR loading data from {csv_filename}: {e}")
    exit()

# 2. Define Target Variable and Features
# --- Uses the 'loan_status' column directly from the merged CSV ---
target_column = 'loan_status'

if target_column not in df.columns:
    print(f"ERROR: Target column '{target_column}' not found in {csv_filename}.")
    print("Available columns:", df.columns.tolist())
    exit()

# Verify target is binary 0/1
# Check distinct values AFTER potential NaN drop below if needed
target_values = df[target_column].dropna().unique()
if not all(v in [0, 1] for v in target_values):
    print(f"Warning: Target column '{target_column}' doesn't seem to be strictly binary (0/1). Found values: {target_values}")
    # Optional: Add forced mapping if necessary, but data should be clean now
    # If mapping fails or unexpected values remain, exit
    # Example check: if df[target_column].isnull().any(): exit("Target column has NaNs after initial load")

# Separate features (X) and target (y)
y = df[target_column]
X = df.drop(target_column, axis=1) # Drop only the target
print(f"\nTarget variable '{target_column}' separated.")
print("Features (X) shape:", X.shape)
print("Target (y) shape:", y.shape)
print("Columns in features X:", X.columns.tolist()) # Verify all expected features are here

# 3. Handle Missing Data (Safety Check)
# Although imputation was done in base.py, this ensures no new NaNs crept in
print(f"\nChecking for missing values before split...")
if X.isnull().any().any():
    print("Warning: Found unexpected NaNs in features (X) after loading merged file. Dropping rows...")
    original_rows_X = X.shape[0]
    rows_to_drop_X_index = X[X.isnull().any(axis=1)].index
    X = X.dropna()
    y = y.drop(rows_to_drop_X_index) # Align y
    print(f"Dropped {original_rows_X - X.shape[0]} rows from X (and y) due to NaNs.")
else: print("No missing values found in features (X).")
if y.isnull().any():
     print("Warning: Found unexpected NaNs in target variable (y). Dropping rows...")
     original_rows_y = y.shape[0]
     y = y.dropna()
     X = X.loc[y.index] # Align X
     print(f"Dropped {original_rows_y - y.shape[0]} rows from y (and X) due to NaNs.")
else: print("No missing values found in target (y).")

# Final shape check
print("Final shapes before split - X:", X.shape, "y:", y.shape)
if X.shape[0] == 0:
    print("ERROR: No data remaining after handling missing values.")
    exit()

# 4. Identify Feature Types
# --- These lists MUST match the columns ACTUALLY PRESENT in merged_credit_data_v2.csv (excluding loan_status) ---
categorical_features = ['employment_status', 'cb_person_default_on_file'] # From users/credit_data
numerical_features = [
    'credit_utilization_ratio',     # From scoring
    'payment_history',              # From scoring
    'loan_term',                    # From scoring
    'person_income',                # From risk
    'loan_amnt',                    # From risk
    'loan_percent_income',          # From risk
    'cb_person_cred_hist_length',   # From risk/users
    'original_loan_amount',         # Created as copy of loan_amnt in base.py
]
# --- ---------------------------------------------------------------------------------------------------------- ---

print("\nIdentified Feature Types:")
print("Categorical:", categorical_features)
print("Numerical:", numerical_features)

# --- Sanity check: Ensure lists cover all columns in X ---
all_defined_features = set(numerical_features + categorical_features)
actual_X_columns = set(X.columns)

missing_in_X = all_defined_features - actual_X_columns
if missing_in_X:
    print(f"ERROR: Features defined in lists but MISSING from the loaded data X: {missing_in_X}")
    exit()

extra_in_X = actual_X_columns - all_defined_features
if extra_in_X:
    print(f"ERROR: Features found in the loaded data X but NOT defined in lists: {extra_in_X}")
    exit()
# --- ------------------------------------------------- ---


# 5. Train-Test Split
print("\nSplitting data into Training (80%) and Testing (20%) sets...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,       # 20% held out for testing
    random_state=42,     # Ensures reproducibility
    stratify=y           # Keeps class proportions same in train/test
)
print("Splitting complete.")
print("X_train shape:", X_train.shape, "| y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape, "| y_test shape:", y_test.shape)
if X_train.shape[0] == 0 or X_test.shape[0] == 0:
     print("ERROR: Train or Test set is empty after split. Check input data size.")
     exit()


# 6. Preprocessing Pipeline (Handles scaling and encoding)
print("\nSetting up preprocessing pipeline...")
# Transformer for numerical features: Scale to range [0, 1]
numerical_transformer = Pipeline(steps=[
    ('scaler', MinMaxScaler())
])

# Transformer for categorical features: Convert categories into one-hot vectors
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# Use ColumnTransformer to apply different transformers to different columns
# This ensures the correct features are selected and processed
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),   # Apply scaling to numerical list
        ('cat', categorical_transformer, categorical_features) # Apply one-hot to categorical list
    ],
    remainder='passthrough' # If any columns in X aren't in num/cat lists, pass them through unchanged (shouldn't happen if lists are correct)
)
print("Preprocessor configured.")


# 7. Model Definition & Training Pipeline
print("\nTraining Random Forest Classifier...")
# Define the Random Forest model
rf_model = RandomForestClassifier(
    n_estimators=100,      # Number of trees in the forest
    random_state=42,       # Ensures reproducibility
    class_weight='balanced', # Adjusts weights for imbalanced classes (like loan default)
    n_jobs=-1              # Use all available CPU cores for training
)

# Create the full pipeline: Preprocessing -> Classifier
# This pipeline applies preprocessing and then trains the model
full_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', rf_model)
])

# Train the entire pipeline on the training data
# Preprocessing steps are fit ONLY on X_train and then transform X_train
# Classifier is trained on the preprocessed X_train
full_pipeline.fit(X_train, y_train)
print("Training complete.")


# 8. Model Evaluation on Test Set
print("\nEvaluating model on the Test set...")
# Use the pipeline to predict on the test data
# Pipeline automatically applies the FITTED preprocessing steps to X_test
y_pred = full_pipeline.predict(X_test)
# --- FIX: Corrected typo from 'pred3ict_proba' to 'predict_proba' ---
y_pred_proba = full_pipeline.predict_proba(X_test)[:, 1] # Get probability of the positive class (1)
# -------------------------------------------------------------------

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
try:
    # AUC requires probabilities and at least two classes in y_test
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"Test Set AUC: {auc:.4f}")
except ValueError as e:
    # Handle error if only one class represented in y_test (unlikely with stratify)
    print(f"Could not calculate AUC (likely only one class in test set labels): {e}")
    auc = "N/A"

print(f"Test Set Accuracy: {accuracy:.4f}")
print("\nTest Set Classification Report:\n", report)


# 9. Saving the Trained Pipeline
print("\nSaving the trained model pipeline...")
# Use a versioned filename consistent with the data used
model_filename = 'credit_model_v2.joblib'
try:
    # Save the ENTIRE pipeline object (preprocessor + classifier)
    joblib.dump(full_pipeline, model_filename)
    print(f"âœ… Trained Pipeline saved successfully to {model_filename}")
except Exception as e:
     print(f"ERROR saving model pipeline: {e}")

print("\n--- Training Script Finished ---")